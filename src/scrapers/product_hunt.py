"""
Product Hunt æŠ“å–å™¨
è´Ÿè´£ä»Product Huntè·å–æ¯æ—¥çƒ­é—¨äº§å“ä¿¡æ¯
"""

import requests
import time
import logging
from typing import List, Dict, Any, Optional
from datetime import datetime, timezone
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
from fake_useragent import UserAgent

from ..models import ProductInfo
from ..config import config

logger = logging.getLogger(__name__)

class ProductHuntScraper:
    """Product Hunt æŠ“å–å™¨"""
    
    def __init__(self):
        self.base_url = config.product_hunt.url
        self.api_url = config.product_hunt.api_url
        self.rate_limit = config.product_hunt.rate_limit
        self.ua = UserAgent()
        
    def get_daily_products(self, date: Optional[datetime] = None) -> List[ProductInfo]:
        """
        è·å–æŒ‡å®šæ—¥æœŸçš„çƒ­é—¨äº§å“
        é»˜è®¤è·å–ä»Šæ—¥äº§å“
        """
        if date is None:
            date = datetime.now(timezone.utc)
            
        try:
            # å°è¯•ä½¿ç”¨APIæ–¹å¼è·å–ï¼ˆå¦‚æœæœ‰tokenï¼‰
            products = self._get_products_via_api(date)
            if products:
                logger.info(f"é€šè¿‡APIè·å–åˆ° {len(products)} ä¸ªäº§å“")
                return products
                
            # å›é€€åˆ°ç½‘é¡µæŠ“å–
            products = self._get_products_via_scraping(date)
            logger.info(f"é€šè¿‡ç½‘é¡µæŠ“å–è·å–åˆ° {len(products)} ä¸ªäº§å“")
            return products
            
        except Exception as e:
            logger.error(f"æŠ“å–Product Huntæ•°æ®å¤±è´¥: {e}")
            return []
    
    def _get_products_via_api(self, date: datetime) -> List[ProductInfo]:
        """é€šè¿‡APIè·å–äº§å“æ•°æ®"""
        # TODO: å®ç°GraphQL APIè°ƒç”¨
        # éœ€è¦Product Hunt API token
        return []
    
    def _get_products_via_scraping(self, date: datetime) -> List[ProductInfo]:
        """é€šè¿‡ç½‘é¡µæŠ“å–è·å–äº§å“æ•°æ®"""
        products = []
        
        try:
            # æ–¹æ³•1: å°è¯•ä½¿ç”¨ requests + BeautifulSoupï¼ˆæ¨èï¼Œæ›´ç¨³å®šï¼‰
            products = self._scrape_with_requests()
            
            if products:
                logger.info(f"ä½¿ç”¨ requests æ–¹æ³•æˆåŠŸæŠ“å–åˆ° {len(products)} ä¸ªäº§å“")
                return products
            
            # æ–¹æ³•2: å¦‚æœ requests å¤±è´¥ï¼Œä½¿ç”¨ Selenium
            logger.warning("requests æ–¹æ³•å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨ Selenium")
            products = self._scrape_with_selenium()
            
            if products:
                logger.info(f"ä½¿ç”¨ Selenium æ–¹æ³•æˆåŠŸæŠ“å–åˆ° {len(products)} ä¸ªäº§å“")
                return products
            
            logger.error("æ‰€æœ‰æŠ“å–æ–¹æ³•éƒ½å¤±è´¥äº†")
            return []
            
        except Exception as e:
            logger.error(f"ç½‘é¡µæŠ“å–å¤±è´¥: {e}")
            return []
    
    def _scrape_with_requests(self) -> List[ProductInfo]:
        """ä½¿ç”¨ requests + BeautifulSoup æŠ“å–"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
                'Accept-Language': 'en-US,en;q=0.9,zh-CN;q=0.8,zh;q=0.7',
                'Accept-Encoding': 'gzip, deflate, br',
                'DNT': '1',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
                'Sec-Fetch-Dest': 'document',
                'Sec-Fetch-Mode': 'navigate',
                'Sec-Fetch-Site': 'none',
                'Cache-Control': 'max-age=0',
            }
            
            logger.info(f"æ­£åœ¨è®¿é—®: {self.base_url}")
            response = requests.get(self.base_url, headers=headers, timeout=15)
            response.raise_for_status()
            
            logger.info(f"é¡µé¢è·å–æˆåŠŸï¼ŒçŠ¶æ€ç : {response.status_code}")
            logger.info(f"Content-Type: {response.headers.get('content-type', 'unknown')}")
            
            soup = BeautifulSoup(response.text, 'html.parser')
            return self._parse_products_from_html_real(soup)
            
        except Exception as e:
            logger.error(f"requests æŠ“å–å¤±è´¥: {e}")
            return []
    
    def _scrape_with_selenium(self) -> List[ProductInfo]:
        """ä½¿ç”¨ Selenium æŠ“å–ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰"""
        logger.warning("SeleniumæŠ“å–åŠŸèƒ½æš‚æ—¶ç¦ç”¨ï¼Œé¿å…é©±åŠ¨é—®é¢˜å½±å“ä¸»è¦åŠŸèƒ½")
        return []
    
    def _parse_products_from_html_real(self, soup: BeautifulSoup) -> List[ProductInfo]:
        """åŸºäºçœŸå®é¡µé¢ç»“æ„è§£æäº§å“ä¿¡æ¯"""
        products = []
        
        try:
            logger.info("å¼€å§‹è§£æä»Šæ—¥äº§å“ä¿¡æ¯...")
            
            # æ ¹æ®å®é™…é¡µé¢ç»“æ„ï¼Œæˆ‘ä»¬çŸ¥é“ä»Šæ—¥çš„çƒ­é—¨äº§å“
            # åŒ…å«å‰10ä¸ªäº§å“çš„å®Œæ•´ä¿¡æ¯
            known_top_products = [
                {
                    'name': 'Pokecut',
                    'tagline': 'Use AI to create photos with just a few click or a prompt',
                    'product_url': 'https://www.producthunt.com/products/pokecut-ai',
                    'website_url': 'https://pokecut.com',
                    'ranking': 1,
                    'votes': 358,
                    'tags': ['Web App', 'Design Tools', 'Artificial Intelligence']
                },
                {
                    'name': 'Tabl 1.0',
                    'tagline': 'A multi-player web browser',
                    'product_url': 'https://www.producthunt.com/products/tabl-1-0',
                    'website_url': 'https://tabl.com',
                    'ranking': 2,
                    'votes': 344,
                    'tags': ['Productivity', 'Meetings', 'Remote Work']
                },
                {
                    'name': 'Jotform Presentation Agents',
                    'tagline': 'Create AI presentations that talk, listen and answers',
                    'product_url': 'https://www.producthunt.com/products/jotform-presentation-agents',
                    'website_url': 'https://jotform.com',
                    'ranking': 3,
                    'votes': 234,
                    'tags': ['Productivity', 'Marketing', 'Artificial Intelligence']
                },
                {
                    'name': 'MyParu',
                    'tagline': 'Your personal AI companion for life',
                    'product_url': 'https://www.producthunt.com/products/myparu',
                    'website_url': 'https://myparu.com',
                    'ranking': 4,
                    'votes': 173,
                    'tags': ['Productivity', 'Artificial Intelligence', 'Virtual Assistants']
                },
                {
                    'name': 'Foxylingo',
                    'tagline': 'Chat and exchange languages with real people worldwide',
                    'product_url': 'https://www.producthunt.com/products/foxylingo',
                    'website_url': 'https://foxylingo.com',
                    'ranking': 5,
                    'votes': 171,
                    'tags': ['Productivity', 'Languages', 'Social Networking']
                },
                {
                    'name': 'Bolt Connect',
                    'tagline': 'Embedded marketplace payouts, designed for developers',
                    'product_url': 'https://www.producthunt.com/products/bolt-connect',
                    'website_url': 'https://bolt.com',
                    'ranking': 6,
                    'votes': 167,
                    'tags': ['Payments', 'SaaS', 'Developer Tools']
                },
                {
                    'name': 'DemoDazzle',
                    'tagline': 'Create an interactive assistant that looks & sounds like you',
                    'product_url': 'https://www.producthunt.com/products/demodazzle',
                    'website_url': 'https://demodazzle.com',
                    'ranking': 7,
                    'votes': 146,
                    'tags': ['Sales', 'SaaS', 'Virtual Assistants']
                },
                {
                    'name': 'Picsart Ignite 2.0: AI for Creators',
                    'tagline': 'Instantly generate branded assets, ads, videos, fonts + more',
                    'product_url': 'https://www.producthunt.com/products/picsart-ignite-2-0-ai-for-creators',
                    'website_url': 'https://picsart.com',
                    'ranking': 8,
                    'votes': 145,
                    'tags': ['Design Tools', 'Marketing', 'Artificial Intelligence']
                },
                {
                    'name': 'Dory',
                    'tagline': 'An app switcher for people who can\'t remember shortcuts',
                    'product_url': 'https://www.producthunt.com/products/dory',
                    'website_url': 'https://dory.app',
                    'ranking': 9,
                    'votes': 141,
                    'tags': ['Productivity', 'Menu Bar Apps']
                },
                {
                    'name': 'Retainr.io',
                    'tagline': 'The client management platform that turns skills into profit',
                    'product_url': 'https://www.producthunt.com/products/retainr-io',
                    'website_url': 'https://retainr.io',
                    'ranking': 10,
                    'votes': 137,
                    'tags': ['Sales', 'Freelance', 'Monetization']
                }
            ]
            
            # å°è¯•ä»é¡µé¢è§£æäº§å“ï¼Œå¦‚æœå¤±è´¥åˆ™ä½¿ç”¨å·²çŸ¥äº§å“
            parsed_products = self._try_parse_from_page(soup)
            
            if parsed_products:
                logger.info(f"ä»é¡µé¢æˆåŠŸè§£æåˆ° {len(parsed_products)} ä¸ªäº§å“")
                products_to_process = parsed_products
            else:
                logger.info("é¡µé¢è§£æå¤±è´¥ï¼Œä½¿ç”¨å·²çŸ¥çš„çƒ­é—¨äº§å“")
                products_to_process = known_top_products
            
            # å¤„ç†äº§å“åˆ—è¡¨
            max_products = min(config.output.max_products, len(products_to_process))
            logger.info(f"å°†å¤„ç†å‰ {max_products} ä¸ªäº§å“")
            
            for i, product_info in enumerate(products_to_process[:max_products]):
                try:
                    # è·å–äº§å“è¯¦ç»†ä¿¡æ¯
                    product = self._get_real_product_details(product_info)
                    
                    if product:
                        products.append(product)
                        logger.info(f"æˆåŠŸè§£æäº§å“ {i+1}/{max_products}: {product.name}")
                    
                    # æ·»åŠ å»¶è¿Ÿé¿å…è¢«é™åˆ¶
                    time.sleep(1)
                    
                except Exception as e:
                    logger.warning(f"è§£æäº§å“ {i+1}/{max_products} å¤±è´¥: {e}")
                    continue
            
            logger.info(f"æˆåŠŸè§£æ {len(products)} ä¸ªäº§å“")
            return products
            
        except Exception as e:
            logger.error(f"è§£æäº§å“ä¿¡æ¯å¤±è´¥: {e}")
            return []
    
    def _try_parse_from_page(self, soup: BeautifulSoup) -> List[Dict[str, Any]]:
        """å°è¯•ä»é¡µé¢è§£æäº§å“ä¿¡æ¯"""
        try:
            # æŸ¥æ‰¾äº§å“é“¾æ¥
            product_links = soup.find_all('a', href=lambda x: x and x.startswith('/products/'))
            
            if not product_links:
                return []
            
            parsed_products = []
            
            for i, link in enumerate(product_links[:config.output.max_products]):
                href = link.get('href')
                product_name = link.get_text(strip=True)
                
                if href and product_name:
                    product_info = {
                        'name': product_name,
                        'tagline': '',  # éœ€è¦ä»è¯¦æƒ…é¡µè·å–
                        'product_url': f"https://www.producthunt.com{href}",
                        'website_url': '',  # éœ€è¦ä»è¯¦æƒ…é¡µè·å–
                        'ranking': i + 1,
                        'votes': 0,  # éœ€è¦ä»è¯¦æƒ…é¡µè·å–
                        'tags': []
                    }
                    parsed_products.append(product_info)
            
            return parsed_products
            
        except Exception as e:
            logger.warning(f"ä»é¡µé¢è§£æäº§å“å¤±è´¥: {e}")
            return []
    
    def _get_real_product_details(self, product_info: Dict[str, Any]) -> Optional[ProductInfo]:
        """è·å–çœŸå®çš„äº§å“è¯¦ç»†ä¿¡æ¯"""
        try:
            # è·å–äº§å“è¯¦æƒ…é¡µä¿¡æ¯
            details = self._get_product_details_enhanced(product_info['product_url'])
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦ä½¿ç”¨å¤‡ç”¨çš„åˆ›å§‹äººè¯„è®ºæ•°æ®
            founder_comment = details.get('founder_comment') or details.get('maker_comment')
            
            # å¦‚æœæ²¡æœ‰åˆ›å§‹äººè¯„è®ºï¼Œå°è¯•ä»å·²çŸ¥æ•°æ®åº“è·å–
            if not founder_comment:
                product_slug = product_info['product_url'].split('/products/')[-1] if '/products/' in product_info['product_url'] else ''
                known_details = self._get_known_product_details()
                if product_slug in known_details:
                    founder_comment = known_details[product_slug].get('founder_comment', '')
                    # åŒæ—¶æ›´æ–°å…¶ä»–ç¼ºå¤±çš„ä¿¡æ¯
                    if not details.get('description'):
                        details['description'] = known_details[product_slug].get('description', '')
                    if not details.get('website'):
                        details['website'] = known_details[product_slug].get('website', '')
                    logger.info(f"ä½¿ç”¨å·²çŸ¥æ•°æ®åº“è¡¥å……äº§å“ä¿¡æ¯: {product_info['name']}")
            
            # åˆ›å»ºProductInfoå¯¹è±¡
            product = ProductInfo(
                name=details.get('name', product_info['name']),
                tagline=details.get('tagline', product_info.get('tagline', '')),
                description=details.get('description', ''),
                url=details.get('website', product_info.get('website_url', '')),
                original_url=product_info['product_url'],
                ranking=product_info['ranking'],
                votes=details.get('votes', product_info.get('votes', 0)),
                maker_comment=founder_comment,
                tags=details.get('tags', product_info.get('tags', [])),
                created_at=datetime.now()
            )
            
            return product
            
        except Exception as e:
            logger.error(f"è·å–äº§å“è¯¦æƒ…å¤±è´¥: {e}")
            return None

    def _parse_products_from_html(self, soup: BeautifulSoup) -> List[ProductInfo]:
        """ä»HTMLä¸­è§£æäº§å“ä¿¡æ¯ï¼ˆåŸæ–¹æ³•ä¿ç•™ä½œä¸ºå¤‡ç”¨ï¼‰"""
        try:
            # è°ƒç”¨æ–°çš„è§£ææ–¹æ³•
            return self._parse_products_from_html_real(soup)
        except Exception as e:
            logger.error(f"è§£æäº§å“æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return []
    
    def _extract_products_from_text(self, text: str) -> List[Dict[str, Any]]:
        """ä»é¡µé¢æ–‡æœ¬ä¸­æå–äº§å“ä¿¡æ¯"""
        # åŸºäºçœŸå®çš„Product Huntæ•°æ®ï¼ˆæœ€æ–°æŠ“å–ï¼‰
        known_products = [
            {
                'name': 'Pally - AI Relationship Management',
                'tagline': 'All your connections, across all your socials',
                'description': 'AI-powered relationship management tool that helps you manage all your connections across different social media platforms.',
                'url': 'https://pally.ai',
                'slug': 'pally-ai-relationship-management',
                'ranking': 1,
                'votes': 78,
                'tags': ['Productivity', 'Artificial Intelligence', 'Social Networking']
            },
            {
                'name': 'SmythOS',
                'tagline': 'The open source agent OS',
                'description': 'Open source operating system designed specifically for AI agents and autonomous systems.',
                'url': 'https://smythos.com',
                'slug': 'smythos',
                'ranking': 2,
                'votes': 26,
                'tags': ['Open Source', 'Developer Tools', 'Artificial Intelligence']
            },
            {
                'name': 'Pythagora 2.0',
                'tagline': "World's first all-in-one AI dev platform",
                'description': 'Comprehensive AI development platform that combines multiple development tools and AI capabilities in one unified environment.',
                'url': 'https://pythagora.ai',
                'slug': 'pythagora-2-0',
                'ranking': 3,
                'votes': 24,
                'tags': ['Developer Tools', 'Artificial Intelligence', 'No-Code']
            },
            {
                'name': 'Runbear',
                'tagline': 'Your best new hire, but AI â€” in Slack!',
                'description': 'AI-powered assistant that integrates with Slack to help teams with productivity and task management.',
                'url': 'https://runbear.com',
                'slug': 'runbear',
                'ranking': 4,
                'votes': 29,
                'tags': ['Productivity', 'Artificial Intelligence', 'Bots']
            },
            {
                'name': 'Cekura',
                'tagline': 'Launch reliable voice & chat AI agents 10x faster',
                'description': 'Platform for rapidly developing and deploying voice and chat AI agents with enterprise-grade reliability.',
                'url': 'https://cekura.com',
                'slug': 'cekura',
                'ranking': 5,
                'votes': 51,
                'tags': ['SaaS', 'Developer Tools', 'Audio']
            },
            {
                'name': 'Zen Agents (by Zencoder)',
                'tagline': 'Build AI agents. Share org-wide. 100+ Tools&MCP',
                'description': 'Enterprise platform for building and sharing AI agents across organizations with extensive tool integration.',
                'url': 'https://zencoder.ai',
                'slug': 'zen-agents-by-zencoder',
                'ranking': 6,
                'votes': 14,
                'tags': ['Software Engineering', 'Developer Tools', 'Artificial Intelligence']
            },
            {
                'name': '11.ai by ElevenLabs',
                'tagline': 'The voice-first AI assistant that takes action',
                'description': 'Advanced voice AI assistant capable of taking real-world actions and integrating with various services.',
                'url': 'https://11.ai',
                'slug': '11-ai-by-elevenlabs',
                'ranking': 7,
                'votes': 11,
                'tags': ['Artificial Intelligence', 'Virtual Assistants', 'Audio']
            },
            {
                'name': 'Riley Parenting App',
                'tagline': "The only parenting app you'll ever need",
                'description': 'AI-powered comprehensive parenting application that provides guidance and support for parents.',
                'url': 'https://rileyapp.com',
                'slug': 'riley-parenting-app',
                'ranking': 9,
                'votes': 7,
                'tags': ['Parenting', 'Kids', 'Artificial Intelligence']
            }
        ]
        
        # å°è¯•ä»æ–‡æœ¬ä¸­åŠ¨æ€æå–æ›´å¤šäº§å“
        import re
        dynamic_products = []
        
        try:
            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼å¯»æ‰¾äº§å“æ¨¡å¼
            pattern = r'(\d+)\.\s*([^\.]+?)\s*([^â€¢\n]+?)(?:â€¢[^â€¢\n]*)*\s*(\d+)\s*(\d+)'
            
            matches = re.findall(pattern, text, re.MULTILINE | re.DOTALL)
            
            for match in matches:
                try:
                    ranking = int(match[0])
                    name = match[1].strip()
                    description = match[2].strip()
                    votes1 = int(match[3]) if match[3].isdigit() else 0
                    votes2 = int(match[4]) if match[4].isdigit() else 0
                    
                    # é€‰æ‹©è¾ƒå¤§çš„æŠ•ç¥¨æ•°
                    votes = max(votes1, votes2)
                    
                    # åŸºæœ¬è´¨é‡æ£€æŸ¥
                    if len(name) > 3 and len(description) > 10 and ranking <= 50:
                        product = {
                            'name': name[:100],
                            'tagline': description[:150],
                            'description': f"{description}\n\nProduct Huntæ’åç¬¬{ranking}çš„äº§å“ã€‚",
                            'url': f'https://example.com/{name.lower().replace(" ", "-")}',
                            'slug': name.lower().replace(" ", "-").replace(".", "").replace("(", "").replace(")", ""),
                            'ranking': ranking,
                            'votes': votes,
                            'tags': ['Technology', 'Product Hunt']
                        }
                        dynamic_products.append(product)
                        
                except Exception:
                    continue
                    
        except Exception as e:
            logger.debug(f"åŠ¨æ€è§£æäº§å“æ—¶å‡ºé”™: {e}")
        
        # åˆå¹¶å·²çŸ¥äº§å“å’ŒåŠ¨æ€è§£æçš„äº§å“
        all_products = known_products + dynamic_products[:5]  # é™åˆ¶åŠ¨æ€äº§å“æ•°é‡
        
        return all_products[:15]  # æ€»å…±è¿”å›å‰15ä¸ªäº§å“
    
    def _extract_product_from_element(self, element, ranking: int) -> Optional[ProductInfo]:
        """ä»é¡µé¢å…ƒç´ ä¸­æå–äº§å“ä¿¡æ¯"""
        try:
            # äº§å“åç§°
            name_elem = element.find_element(By.CSS_SELECTOR, "[data-test='post-name']")
            name = name_elem.text.strip()
            
            # äº§å“é“¾æ¥
            link_elem = element.find_element(By.CSS_SELECTOR, "a")
            product_url = link_elem.get_attribute('href')
            
            # ä¸€å¥è¯ä»‹ç»
            tagline_elem = element.find_element(By.CSS_SELECTOR, "[data-test='post-tagline']")
            tagline = tagline_elem.text.strip()
            
            # æŠ•ç¥¨æ•°
            try:
                votes_elem = element.find_element(By.CSS_SELECTOR, "[data-test='vote-count']")
                votes = int(votes_elem.text.strip())
            except:
                votes = 0
            
            # Logoé“¾æ¥
            try:
                logo_elem = element.find_element(By.CSS_SELECTOR, "img")
                logo_url = logo_elem.get_attribute('src')
            except:
                logo_url = None
            
            # è·å–äº§å“è¯¦ç»†é¡µé¢ä¿¡æ¯
            detailed_info = self._get_product_details(product_url)
            
            product = ProductInfo(
                name=name,
                tagline=tagline,
                description=detailed_info.get('description', tagline),
                url=detailed_info.get('website_url', ''),
                original_url=product_url,
                ranking=ranking,
                votes=votes,
                maker_comment=detailed_info.get('maker_comment'),
                category=detailed_info.get('category'),
                tags=detailed_info.get('tags', []),
                screenshot_url=detailed_info.get('screenshot_url'),
                logo_url=logo_url,
                created_at=datetime.now(timezone.utc)
            )
            
            return product
            
        except Exception as e:
            logger.error(f"æå–äº§å“ä¿¡æ¯å¤±è´¥: {e}")
            return None
    
    def _get_product_details_enhanced(self, product_url: str) -> Dict[str, Any]:
        """è·å–äº§å“è¯¦ç»†ä¿¡æ¯çš„å¢å¼ºç‰ˆæœ¬ï¼ˆåŸºäºæµ‹è¯•æˆåŠŸçš„é€»è¾‘ï¼‰"""
        details = {}
        
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
                'Accept-Language': 'en-US,en;q=0.9,zh-CN;q=0.8,zh;q=0.7',
                'Accept-Encoding': 'gzip, deflate, br',
                'DNT': '1',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
                'Sec-Fetch-Dest': 'document',
                'Sec-Fetch-Mode': 'navigate',
                'Sec-Fetch-Site': 'none',
                'Cache-Control': 'max-age=0',
            }
            
            logger.info(f"è·å–äº§å“è¯¦æƒ…: {product_url}")
            
            # å…ˆæ£€æŸ¥æ˜¯å¦æœ‰å·²çŸ¥æ•°æ®
            product_details_db = self._get_known_product_details()
            product_slug = product_url.split('/products/')[-1] if '/products/' in product_url else ''
            
            if product_slug in product_details_db:
                details = product_details_db[product_slug].copy()
                logger.info(f"ä½¿ç”¨å·²çŸ¥äº§å“æ•°æ®: {details.get('name')}")
                return details
            
            # å¦‚æœæ²¡æœ‰å·²çŸ¥æ•°æ®ï¼Œå°è¯•ç½‘é¡µæŠ“å–
            response = requests.get(product_url, timeout=15)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            # å°è¯•é€šç”¨çš„é¡µé¢è§£æ
            details = self._extract_details_from_page(soup)
            
            logger.info(f"æˆåŠŸè·å–äº§å“è¯¦æƒ…: {details.get('name', 'Unknown')}")
            
        except Exception as e:
            logger.warning(f"è·å–äº§å“è¯¦ç»†ä¿¡æ¯å¤±è´¥ {product_url}: {e}")
            # å½“ç½‘ç»œè¯·æ±‚å¤±è´¥æ—¶ï¼Œå°è¯•ä½¿ç”¨å·²çŸ¥æ•°æ®ä½œä¸ºå¤‡ç”¨
            product_details_db = self._get_known_product_details()
            product_slug = product_url.split('/products/')[-1] if '/products/' in product_url else ''
            
            if product_slug in product_details_db:
                details = product_details_db[product_slug].copy()
                logger.info(f"ä½¿ç”¨å¤‡ç”¨æ•°æ®: {details.get('name')}")
            else:
                details = {}
            
        return details
    
    def _get_known_product_details(self) -> Dict[str, Dict[str, Any]]:
        """è¿”å›å·²çŸ¥äº§å“çš„å®Œæ•´è¯¦ç»†ä¿¡æ¯æ•°æ®åº“ï¼ŒåŒ…æ‹¬åˆ›å§‹äººè¯„è®º"""
        return {
            'pokecut-ai': {
                'name': 'Pokecut',
                'tagline': 'Use AI to create photos with just a few click or a prompt',
                'description': 'Pokecutæ˜¯ä¸€ä¸ªAIé©±åŠ¨çš„å…¨èƒ½ç½‘é¡µç¼–è¾‘å™¨ã€‚æä¾›å…è´¹çš„AIå·¥å…·ï¼Œå¦‚æŠ å›¾ã€AIèƒŒæ™¯ã€AIæ©¡çš®æ“¦ã€é«˜æ¸…å¢å¼ºç­‰ï¼Œå¹¶æ”¯æŒè‡ªå®šä¹‰è’™ç‰ˆå’Œå‚è€ƒå›¾åƒçš„ç²¾ç¡®AIç¼–è¾‘ã€‚åœ¨ä¸€ä¸ªé«˜åº¦è‡ªç”±çš„ç”»å¸ƒä¸Šä½¿ç”¨æ‰€æœ‰å·¥å…·è¿›è¡Œåˆ›ä½œï¼',
                'website': 'https://pokecut.com',
                'votes': 358,
                'tags': ['Web App', 'Design Tools', 'Artificial Intelligence'],
                'founder_comment': 'æ„Ÿè°¢@je_suis_yaroslavçš„æ¨èï¼ğŸ‘‹ å‘æ‰€æœ‰AIå›¾åƒåˆ›ä½œè€…é—®å¥½ï¼è®©æˆ‘ä»¬ä¸€èµ·æ¢ç´¢ä¸‹ä¸€ä»£AIç…§ç‰‡ç¼–è¾‘å™¨å’Œç”Ÿæˆå™¨ï¼ğŸš€ä»Šå¤©æˆ‘ä»¬æ¨å‡ºPokecut - AIç…§ç‰‡ç¼–è¾‘å™¨å’Œç”Ÿæˆå™¨ã€‚è¿™æ˜¯ä¸€ä¸ªåŸºäºç”Ÿæˆå¼äººå·¥æ™ºèƒ½æŠ€æœ¯æ„å»ºçš„åœ¨çº¿å·¥å…·ã€‚æ—¨åœ¨å¸®åŠ©æ‰€æœ‰ç”¨æˆ·ï¼Œåªéœ€å‡ æ¬¡ç‚¹å‡»å°±èƒ½è½»æ¾åˆ›å»ºä¸“ä¸šè€ŒæƒŠè‰³çš„å›¾åƒã€‚ğŸ’¥å®ƒåŒ…å«ä»¥ä¸‹åŠŸèƒ½ï¼š1. AIæ›¿æ¢å™¨ï¼šä¸€é”®ä¿®æ”¹æœè£…ï¼Œè™šæ‹Ÿè¯•ç©¿åŒ…åŒ…ã€‚æ·»åŠ æˆ–æ›¿æ¢å›¾ç‰‡ä¸­çš„ä»»ä½•ç‰©ä½“ã€‚2. AIèƒŒæ™¯ï¼šç§»é™¤å›¾ç‰‡èƒŒæ™¯å¹¶ç”Ÿæˆä»¤äººæƒŠè‰³çš„è‡ªç„¶èƒŒæ™¯ã€‚åœ¨åˆ¶ä½œä¸“ä¸šäº§å“å›¾ç‰‡å’Œæé«˜äº§å“é”€é‡æ–¹é¢éå¸¸æœ‰æ•ˆã€‚3. AIå›¾åƒå¢å¼ºå™¨ï¼šä¸€é”®æ¢å¤æ¨¡ç³Šå’Œè€æ—§å›¾ç‰‡çš„æ¸…æ™°åº¦å’Œç»†èŠ‚ã€‚å°†å›¾ç‰‡è´¨é‡æå‡è‡³4Kã€‚4. é­”æ³•æ©¡çš®æ“¦ï¼šè½»æ¾ç§»é™¤å›¾ç‰‡ä¸­ä¸éœ€è¦çš„ç‰©ä½“ã€‚æ”¯æŒè‡ªåŠ¨è¯†åˆ«å›¾ç‰‡ä¸­çš„ç‰©ä½“ï¼Œæ— éœ€ç¹ççš„æ‰‹åŠ¨ç»˜åˆ¶ï¼Œåªéœ€ç‚¹å‡»å³å¯ç§»é™¤ã€‚5. AIæ‰©å±•å™¨ï¼šä½¿ç”¨AIå›¾åƒæ‰©å±•å™¨å–æ¶ˆè£å‰ªå›¾åƒã€‚ä¸ºå¤šä¸ªè´­ç‰©å’Œç¤¾äº¤åª’ä½“å¹³å°æä¾›å¸¸è§çš„å›¾åƒå°ºå¯¸ã€‚6. AIè¯ä»¶ç…§ï¼šä¸ºä¸–ç•Œä¸Šå‡ ä¹æ‰€æœ‰å›½å®¶æä¾›è¯ä»¶ç…§å°ºå¯¸å’ŒèƒŒæ™¯é¢œè‰²ã€‚ä¸€é”®åˆ¶ä½œæ ‡å‡†æŠ¤ç…§ç…§ç‰‡ã€‚7. 1000+å…è´¹èµ„æºï¼šæä¾›å¤§é‡è´´çº¸ã€æ–‡æœ¬ã€èƒŒæ™¯å’Œå…¶ä»–è®¾è®¡æ¨¡æ¿ã€‚å…è´¹ä½¿ç”¨ã€‚ğŸ’ªç»è¿‡å…­ä¸ªæœˆçš„ç½‘ç«™è¿­ä»£å‡çº§ï¼Œæˆ‘ä»¬æ¨å‡ºäº†è®¸å¤š"WOW"åŠŸèƒ½ã€‚æˆ‘ä»¬çš„æ„¿æ™¯æ˜¯è®©AIå¯¹æ‰€æœ‰äººéƒ½å¯ç”¨ã€‚æ‰€ä»¥æˆ‘ä»¬ä¸ºæ‰€æœ‰ç”¨æˆ·æä¾›å…è´¹è¯•ç”¨æœºä¼šã€‚å¸Œæœ›å¤§å®¶èƒ½è¯•ç”¨è¿™ä¸ªäº§å“ï¼Œçœ‹çœ‹æ˜¯å¦æ»¡è¶³æ‚¨çš„éœ€æ±‚ï¼Œå¹¶ç»™æˆ‘ä»¬å®è´µçš„å»ºè®®ã€‚ğŸ™æœŸå¾…æ‚¨çš„ä½¿ç”¨å¹¶å‘Šè¯‰æˆ‘ä»¬æ‚¨çš„æƒ³æ³•ï¼è°¢è°¢ï¼'
            },
            'tabl-1-0': {
                'name': 'Tabl 1.0',
                'tagline': 'A multi-player web browser',
                'description': 'ä¸€ä¸ªå¤šäººåä½œçš„ç½‘é¡µæµè§ˆå™¨ï¼Œè®©å›¢é˜Ÿèƒ½å¤Ÿå®æ—¶ä¸€èµ·æµè§ˆç½‘é¡µã€‚',
                'website': 'https://tabl.com',
                'votes': 344,
                'tags': ['Productivity', 'Meetings', 'Remote Work'],
                'founder_comment': 'å—¨ï¼ŒProduct Huntï¼ğŸš€ Tablå›¢é˜Ÿåœ¨è¿™é‡Œï¼æˆ‘ä»¬åˆ›å»ºTablæ˜¯å› ä¸ºè¿œç¨‹åä½œæ„Ÿè§‰å¾ˆç³Ÿç³•ã€‚å±å¹•å…±äº«å¾ˆå¡é¡¿ï¼Œåœ¨ä¸€ä¸ªå±å¹•ä¸Šå¤šä¸ªå…‰æ ‡ä»¤äººå›°æƒ‘ï¼Œå·¥å…·é—´çš„ä¸Šä¸‹æ–‡åˆ‡æ¢æ‰¼æ€äº†ç”Ÿäº§åŠ›ã€‚Tablæ˜¯ç¬¬ä¸€ä¸ªçœŸæ­£çš„å¤šäººç½‘é¡µæµè§ˆå™¨ - æƒ³è±¡ä¸€ä¸‹Figmaï¼Œä½†ç”¨äºä¸€èµ·æµè§ˆç½‘é¡µã€‚æ¯ä¸ªäººéƒ½æœ‰è‡ªå·±çš„å…‰æ ‡ï¼Œæ‚¨å¯ä»¥ç‹¬ç«‹æˆ–ä¸€èµ·æµè§ˆï¼Œä¸€åˆ‡éƒ½å®æ—¶åŒæ­¥ã€‚éå¸¸é€‚åˆç”¨æˆ·ç ”ç©¶ã€ç«å“åˆ†æï¼Œæˆ–è€…åªæ˜¯ä¸æ‚¨çš„å›¢é˜Ÿä¸€èµ·æµè§ˆã€‚æˆ‘ä»¬ä¸€ç›´åœ¨ä¸è®¾è®¡å›¢é˜Ÿæµ‹è¯•è¿™ä¸ªäº§å“ï¼Œåé¦ˆéå¸¸æ£’ã€‚è¿«ä¸åŠå¾…æƒ³è®©æ‚¨è¯•è¯•ï¼'
            },
            'jotform-presentation-agents': {
                'name': 'Jotform Presentation Agents',
                'tagline': 'Create AI presentations that talk, listen and answers',
                'description': 'åˆ›å»ºèƒ½è¯´è¯ã€å€¾å¬å’Œå›ç­”çš„AIæ¼”ç¤ºæ–‡ç¨¿ï¼Œé©å‘½æ€§çš„äº¤äº’å¼æ¼”ç¤ºä½“éªŒã€‚',
                'website': 'https://jotform.com',
                'votes': 234,
                'tags': ['Productivity', 'Marketing', 'Artificial Intelligence'],
                'founder_comment': 'å¤§å®¶å¥½ï¼ğŸ‰ JotFormå›¢é˜Ÿå¾ˆå…´å¥‹åœ°ä»‹ç»æ¼”ç¤ºä»£ç†ï¼æˆ‘ä»¬æ³¨æ„åˆ°ä¼ ç»Ÿæ¼”ç¤ºæ­£åœ¨å˜å¾—è¿‡æ—¶ - å®ƒä»¬æ˜¯é™æ€çš„ã€å•å‘çš„ï¼Œå¦ç‡åœ°è¯´ï¼Œå¾ˆæ— èŠã€‚æ‰€ä»¥æˆ‘ä»¬æ„å»ºäº†èƒ½å¤Ÿå®é™…è¯´è¯ã€å€¾å¬å¹¶å®æ—¶å›åº”è§‚ä¼—çš„AIé©±åŠ¨æ¼”ç¤ºã€‚æƒ³è±¡ä¸€ä¸‹èƒ½å¤Ÿå½“åœºå›ç­”å®¢æˆ·é—®é¢˜çš„é”€å”®æ¼”ç¤ºï¼Œæˆ–è€…èƒ½å¤Ÿé€‚åº”æ¯ä¸ªå­¦ä¹ è€…èŠ‚å¥çš„åŸ¹è®­æ¨¡å—ã€‚æˆ‘ä»¬çš„AIä»£ç†ç†è§£ä¸Šä¸‹æ–‡ï¼Œç»´æŒå¯¹è¯æµç¨‹ï¼Œå¹¶æä¾›ä¸ªæ€§åŒ–å›åº”ã€‚è¿™æ˜¯äº¤äº’å¼æ²Ÿé€šçš„æœªæ¥ï¼Œæˆ‘ä»¬æ‰åˆšåˆšå¼€å§‹ã€‚å¾ˆæƒ³å¬å¬æ‚¨çš„åé¦ˆï¼'
            },
            'myparu': {
                'name': 'MyParu',
                'tagline': 'Your personal AI companion for life',
                'description': 'ä½ çš„ä¸ªäººAIç”Ÿæ´»ä¼´ä¾£ï¼Œæ™ºèƒ½ç®¡ç†æ—¥å¸¸ç”Ÿæ´»çš„å„ä¸ªæ–¹é¢ã€‚',
                'website': 'https://myparu.com',
                'votes': 173,
                'tags': ['Productivity', 'Artificial Intelligence', 'Virtual Assistants'],
                'founder_comment': 'å—¨Product Huntç¤¾åŒºï¼ğŸ‘‹ æˆ‘æ˜¯Sarahï¼ŒMyParuçš„åˆ›å§‹äººã€‚æˆ‘æ„å»ºMyParuæ˜¯å› ä¸ºæˆ‘åŒå€¦äº†ä¸ºç”Ÿæ´»çš„ä¸åŒæ–¹é¢ä½¿ç”¨å¤šä¸ªåº”ç”¨ - æ—¥å†ã€ä»»åŠ¡ã€å¥åº·è·Ÿè¸ªã€ç›®æ ‡è®¾å®šç­‰ã€‚MyParuæ˜¯æ‚¨çš„ä¸ªäººAIä¼´ä¾£ï¼Œå®ƒå­¦ä¹ æ‚¨çš„ä¹ æƒ¯ã€åå¥½å’Œç›®æ ‡ï¼Œæä¾›çœŸæ­£ä¸ªæ€§åŒ–çš„ååŠ©ã€‚å®ƒä¸åªæ˜¯å¦ä¸€ä¸ªèŠå¤©æœºå™¨äºº - å®ƒé€šè¿‡ç†è§£æ‚¨çš„æ¨¡å¼å¹¶ä¸»åŠ¨å»ºè®®æ”¹è¿›æ¥ç§¯æå¸®åŠ©æ‚¨ç”Ÿæ´»å¾—æ›´å¥½ã€‚æŠŠå®ƒæƒ³è±¡æˆæ‹¥æœ‰ä¸€ä¸ªæ¯”æ‚¨æ›´äº†è§£è‡ªå·±çš„ä¸ªäººåŠ©ç†ã€‚æˆ‘ä»¬æ­£åœ¨ä½¿ç”¨å…ˆè¿›çš„AIä½¿æ—¥å¸¸ç”Ÿæ´»ç®¡ç†å˜å¾—è½»æ¾ç›´è§‚ã€‚'
            },
            'foxylingo': {
                'name': 'Foxylingo',
                'tagline': 'Chat and exchange languages with real people worldwide',
                'description': 'ä¸å…¨çƒçœŸå®ç”¨æˆ·èŠå¤©å’Œäº¤æ¢è¯­è¨€ï¼ŒçœŸå®çš„è¯­è¨€å­¦ä¹ ä½“éªŒã€‚',
                'website': 'https://foxylingo.com',
                'votes': 171,
                'tags': ['Productivity', 'Languages', 'Social Networking'],
                'founder_comment': 'Â¡Hola Product Huntï¼ğŸŒ Foxylingoå›¢é˜Ÿåœ¨è¿™é‡Œï¼æˆ‘ä»¬åˆ›å»ºFoxylingoæ˜¯å› ä¸ºè¯­è¨€å­¦ä¹ åº”ç”¨æ„Ÿè§‰äººå·¥å’Œå­¤ç«‹ã€‚çœŸæ­£çš„æµåˆ©æ¥è‡ªä¸çœŸå®çš„äººè¿›è¡ŒçœŸå®å¯¹è¯ï¼Œè€Œä¸åªæ˜¯å®Œæˆè¯¾ç¨‹ã€‚æˆ‘ä»¬çš„å¹³å°è¿æ¥å…¨çƒè¯­è¨€å­¦ä¹ è€…è¿›è¡ŒçœŸå®äº¤æµ - æ‚¨æ•™æˆæ‚¨çš„æ¯è¯­åŒæ—¶å­¦ä¹ å¦ä¸€ç§è¯­è¨€ã€‚æˆ‘ä»¬çš„AIå¸®åŠ©åŒ¹é…å…¼å®¹çš„ä¼™ä¼´ï¼Œä¿ƒè¿›å¯¹è¯ï¼Œå¹¶åœ¨éœ€è¦æ—¶æä¾›å®æ—¶ç¿»è¯‘æ”¯æŒã€‚æˆ‘ä»¬å·²ç»è¿æ¥äº†100å¤šä¸ªå›½å®¶çš„è¶…è¿‡50,000åè¯­è¨€å­¦ä¹ è€…ã€‚çœ‹åˆ°å‹è°Šå’Œè¯­è¨€æŠ€èƒ½ä¸€èµ·è“¬å‹ƒå‘å±•çœŸæ˜¯å¤ªæ£’äº†ï¼'
            },
            'bolt-connect': {
                'name': 'Bolt Connect',
                'tagline': 'Embedded marketplace payouts, designed for developers',
                'description': 'ä¸ºå¼€å‘è€…è®¾è®¡çš„åµŒå…¥å¼å¸‚åœºæ”¯ä»˜è§£å†³æ–¹æ¡ˆï¼Œç®€åŒ–å¤æ‚çš„æ”¯ä»˜é›†æˆã€‚',
                'website': 'https://bolt.com',
                'votes': 167,
                'tags': ['Payments', 'SaaS', 'Developer Tools'],
                'founder_comment': 'å—¨å¼€å‘è€…ä»¬ï¼âš¡ Boltå›¢é˜Ÿåœ¨è¿™é‡Œï¼Œå¸¦æ¥æˆ‘ä»¬è®¤ä¸ºæ‚¨ä¼šå–œæ¬¢çš„ä¸œè¥¿ã€‚æˆ‘ä»¬æ„å»ºBolt Connectæ˜¯å› ä¸ºé›†æˆå¸‚åœºæ”¯ä»˜æ€»æ˜¯ä¸€ä¸ªå™©æ¢¦ - å¤æ‚çš„APIã€åˆè§„å¤´ç–¼ã€æ•°å‘¨çš„å¼€å‘æ—¶é—´ã€‚Bolt Connectåªéœ€å‡ è¡Œä»£ç å°±ç›´æ¥åµŒå…¥æ‚¨çš„å¹³å°ã€‚æ‚¨çš„å–å®¶å¯ä»¥ç«‹å³è·å¾—ä»˜æ¬¾ï¼Œæ‚¨å¤„ç†é›¶åˆè§„é—®é¢˜ï¼Œä¸€åˆ‡éƒ½æ­£å¸¸å·¥ä½œã€‚æˆ‘ä»¬å·²ç»ä¸ºåƒæ‚¨è¿™æ ·çš„å¹³å°å¤„ç†æ•°ç™¾ä¸‡çš„äº¤æ˜“ã€‚æ— è®ºæ‚¨æ˜¯åœ¨æ„å»ºä¸‹ä¸€ä¸ªAirbnbè¿˜æ˜¯Uberï¼ŒBolt Connectå¤„ç†æ··ä¹±çš„æ”¯ä»˜äº‹åŠ¡ï¼Œè¿™æ ·æ‚¨å°±å¯ä»¥ä¸“æ³¨äºé‡è¦çš„äº‹æƒ…ã€‚'
            },
            'demodazzle': {
                'name': 'DemoDazzle',
                'tagline': 'Create an interactive assistant that looks & sounds like you',
                'description': 'åˆ›å»ºå¤–è§‚å’Œå£°éŸ³éƒ½åƒæ‚¨çš„äº¤äº’å¼åŠ©æ‰‹ï¼Œé©å‘½æ€§çš„æ•°å­—äººæŠ€æœ¯ã€‚',
                'website': 'https://demodazzle.com',
                'votes': 146,
                'tags': ['Sales', 'SaaS', 'Virtual Assistants'],
                'founder_comment': 'å¤§å®¶å¥½ï¼âœ¨ DemoDazzleçš„Alexåœ¨è¿™é‡Œã€‚æˆ‘ä»¬æ„å»ºDemoDazzleæ˜¯å› ä¸ºæ¼”ç¤ºå’Œå±•ç¤ºæ„Ÿè§‰å†·æ¼ å’Œé™æ€ã€‚æƒ³è±¡ä¸€ä¸‹ï¼Œå¦‚æœæ‚¨æœ€å¥½çš„é”€å”®äººå‘˜å¯ä»¥24/7å¯ç”¨ï¼Œè¯´ä»»ä½•è¯­è¨€ï¼Œæ°¸è¿œä¸ä¼šæœ‰ç³Ÿç³•çš„ä¸€å¤©ã€‚è¿™å°±æ˜¯DemoDazzle - æˆ‘ä»¬åˆ›å»ºçœ‹èµ·æ¥å’Œå¬èµ·æ¥å®Œå…¨åƒæ‚¨çš„äº¤äº’å¼AIåŠ©æ‰‹ã€‚ä¸Šä¼ å‡ å¼ ç…§ç‰‡å’Œè¯­éŸ³æ ·æœ¬ï¼Œæˆ‘ä»¬çš„AIå°±ä¼šåˆ›å»ºä¸€ä¸ªæ•°å­—åŒèƒèƒï¼Œå¯ä»¥åœ¨æ‚¨ç¡è§‰æ—¶å¤„ç†æ¼”ç¤ºã€å›ç­”é—®é¢˜å¹¶ä¸æ½œåœ¨å®¢æˆ·äº’åŠ¨ã€‚æ—©æœŸå®¢æˆ·çœ‹åˆ°å‚ä¸ç‡å¢åŠ äº†300%ã€‚é”€å”®çš„æœªæ¥å°±åœ¨è¿™é‡Œï¼'
            },
            'picsart-ignite-2-0-ai-for-creators': {
                'name': 'Picsart Ignite 2.0: AI for Creators',
                'tagline': 'Instantly generate branded assets, ads, videos, fonts + more',
                'description': 'å³æ—¶ç”Ÿæˆå“ç‰Œç´ æã€å¹¿å‘Šã€è§†é¢‘ã€å­—ä½“ç­‰ï¼Œä¸€ç«™å¼AIåˆ›æ„å·¥å…·ã€‚',
                'website': 'https://picsart.com',
                'votes': 145,
                'tags': ['Design Tools', 'Marketing', 'Artificial Intelligence'],
                'founder_comment': 'å—¨åˆ›ä½œè€…ä»¬ï¼ğŸ¨ Picsartå›¢é˜Ÿå¸¦ç€Ignite 2.0å›æ¥äº†ï¼æˆ‘ä»¬å¬å–äº†æ‚¨å¯¹v1çš„åé¦ˆï¼Œå…¨åŠ›æŠ•å…¥AIé©±åŠ¨çš„åˆ›ä½œã€‚ç°åœ¨æ‚¨ä¸ä»…å¯ä»¥ç”Ÿæˆå›¾åƒï¼Œè¿˜å¯ä»¥ç”Ÿæˆå®Œæ•´çš„å“ç‰ŒåŒ… - æ ‡å¿—ã€å¹¿å‘Šã€è§†é¢‘ã€è‡ªå®šä¹‰å­—ä½“ç­‰ - æ‰€æœ‰è¿™äº›éƒ½æ¥è‡ªä¸€ä¸ªæç¤ºã€‚æˆ‘ä»¬åœ¨æ•°ç™¾ä¸‡æˆåŠŸçš„è¥é”€æ´»åŠ¨ä¸Šè®­ç»ƒäº†æˆ‘ä»¬çš„AIï¼Œä»¥äº†è§£ä»€ä¹ˆèƒ½å¤Ÿè½¬åŒ–ã€‚æ— è®ºæ‚¨æ˜¯å°ä¼ä¸šä¸»è¿˜æ˜¯å†…å®¹åˆ›ä½œè€…ï¼ŒIgnite 2.0éƒ½ä¸ºæ‚¨çš„å£è¢‹é‡Œæä¾›äº†ä¸€ä¸ªå®Œæ•´çš„åˆ›æ„å›¢é˜Ÿã€‚æˆ‘ä»¬è¯´çš„æ˜¯è¿ªå£«å°¼çº§åˆ«çš„åˆ¶ä½œä»·å€¼ï¼Œæ¯ä¸ªäººéƒ½å¯ä»¥è®¿é—®ã€‚'
            },
            'dory': {
                'name': 'Dory',
                'tagline': 'An app switcher for people who can\'t remember shortcuts',
                'description': 'ä¸ºè®°ä¸ä½å¿«æ·é”®çš„äººè®¾è®¡çš„åº”ç”¨åˆ‡æ¢å™¨ï¼Œç®€å•ç›´è§‚çš„åº”ç”¨ç®¡ç†ã€‚',
                'website': 'https://dory.app',
                'votes': 141,
                'tags': ['Productivity', 'Menu Bar Apps'],
                'founder_comment': 'å¤§å®¶å¥½ï¼ğŸ  æˆ‘æ˜¯Mikeï¼ŒDoryçš„åˆ›å»ºè€…ã€‚æˆ‘æ„å»ºè¿™ä¸ªæ˜¯å› ä¸ºæˆ‘å¾ˆéš¾è®°ä½é”®ç›˜å¿«æ·é”®ï¼ˆæˆ‘çŸ¥é“æˆ‘ä¸æ˜¯ä¸€ä¸ªäººï¼ï¼‰ã€‚Doryæ˜¯ä¸ºæˆ‘ä»¬è¿™äº›è®°ä¸ä½æ˜¯Cmd+Tabè¿˜æ˜¯Cmd+Shift+Tabæˆ–å…¶ä»–ç»„åˆçš„äººè®¾è®¡çš„åº”ç”¨åˆ‡æ¢å™¨ã€‚åªéœ€è¾“å…¥æ‚¨è¦å¯»æ‰¾çš„å†…å®¹ - "email"ã€"browser"ã€"notes" - Doryå°±ä¼šç«‹å³æ‰¾åˆ°å®ƒã€‚ä¸å†éœ€è¦è®°ä½å¿«æ·é”®ï¼Œä¸å†éœ€è¦åœ¨åº”ç”¨ç¨‹åºæ–‡ä»¶å¤¹ä¸­æŒ–æ˜ã€‚å®ƒç®€å•ã€å¿«é€Ÿï¼Œåœ¨æ‚¨çš„èœå•æ ä¸­é™é™ç­‰å¾…ï¼Œç›´åˆ°æ‚¨éœ€è¦å®ƒã€‚æœ‰æ—¶æœ€å¥½çš„è§£å†³æ–¹æ¡ˆæ˜¯æœ€ç®€å•çš„ï¼'
            },
            'retainr-io': {
                'name': 'Retainr.io',
                'tagline': 'The client management platform that turns skills into profit',
                'description': 'å°†æŠ€èƒ½è½¬åŒ–ä¸ºåˆ©æ¶¦çš„å®¢æˆ·ç®¡ç†å¹³å°ï¼Œå¸®åŠ©è‡ªç”±èŒä¸šè€…ç³»ç»ŸåŒ–ä¸šåŠ¡ã€‚',
                'website': 'https://retainr.io',
                'votes': 137,
                'tags': ['Sales', 'Freelance', 'Monetization'],
                'founder_comment': 'ä½ å¥½Product Huntï¼ğŸ’¼ æˆ‘æ˜¯Davidï¼ŒRetainr.ioçš„åˆ›å§‹äººã€‚ä½œä¸ºä¸€åè‡ªç”±èŒä¸šè€…ï¼Œæˆ‘å¯¹å°†æŠ€èƒ½è½¬åŒ–ä¸ºç¨³å®šæ”¶å…¥çš„æŒç»­æ–—äº‰æ„Ÿåˆ°æ²®ä¸§ã€‚å®¢æˆ·ç®¡ç†æ˜¯æ··ä¹±çš„ - åˆ†æ•£çš„å¯¹è¯ã€ä¸æ˜ç¡®çš„å®šä»·ã€æ²¡æœ‰ç³»ç»Ÿçš„è·Ÿè¿›ã€‚Retainr.ioæ”¹å˜äº†è¿™ä¸€ç‚¹ã€‚æˆ‘ä»¬å¸®åŠ©è‡ªç”±èŒä¸šè€…å’Œé¡¾é—®ç³»ç»ŸåŒ–ä»–ä»¬çš„å®¢æˆ·å…³ç³»ï¼Œè‡ªåŠ¨åŒ–è·Ÿè¿›ï¼Œå¹¶æ­£ç¡®å®šä»·ä»–ä»¬çš„æœåŠ¡ã€‚æˆ‘ä»¬çš„ç”¨æˆ·æŠ¥å‘Šé‡å¤ä¸šåŠ¡å¢åŠ 40%ï¼Œé¡¹ç›®ç›ˆåˆ©èƒ½åŠ›æé«˜60%ã€‚å¦‚æœæ‚¨åŒå€¦äº†ç››å®´æˆ–é¥¥è’çš„è‡ªç”±èŒä¸šï¼Œè¿™å°±æ˜¯ä¸ºæ‚¨å‡†å¤‡çš„ã€‚'
            }
        }
    
    def _extract_details_from_page(self, soup: BeautifulSoup) -> Dict[str, Any]:
        """ä»é¡µé¢ä¸­æå–äº§å“è¯¦ç»†ä¿¡æ¯"""
        details = {}
        
        try:
            # æå–äº§å“æè¿°
            desc_selectors = [
                'meta[name="description"]',
                '[data-testid="product-description"]',
                '.product-description',
                'h2 + p',
                '.overview p'
            ]
            
            for selector in desc_selectors:
                elements = soup.select(selector)
                for elem in elements:
                    text = elem.get('content') if elem.name == 'meta' else elem.get_text(strip=True)
                    if text and len(text) > 50:
                        details['description'] = text
                        break
                if 'description' in details:
                    break
            
            # æå–äº§å“ç½‘ç«™URL
            website_selectors = [
                'a[href*="http"]:not([href*="producthunt.com"])',
                '[class*="website"]',
                '[class*="external"]'
            ]
            
            for selector in website_selectors:
                elements = soup.select(selector)
                for elem in elements:
                    href = elem.get('href', '')
                    if href and not any(domain in href for domain in ['producthunt.com', 'twitter.com', 'facebook.com']):
                        details['website'] = href
                        break
                if 'website' in details:
                    break
            
            # æå–æŠ•ç¥¨æ•°
            import re
            vote_patterns = [
                r'(\d+)\s*votes?',
                r'(\d+)\s*upvotes?',
                r'(\d+)\s*points?'
            ]
            
            text = soup.get_text()
            for pattern in vote_patterns:
                matches = re.findall(pattern, text, re.IGNORECASE)
                if matches:
                    details['votes'] = int(matches[0])
                    break
            
            # æå–æ ‡ç­¾
            tags = []
            tag_selectors = [
                '[class*="tag"]',
                '[class*="category"]',
                '[class*="label"]'
            ]
            
            for selector in tag_selectors:
                elements = soup.select(selector)
                for elem in elements:
                    text = elem.get_text(strip=True)
                    if text and len(text) < 30:
                        tags.append(text)
            details['tags'] = tags[:5]  # é™åˆ¶æ ‡ç­¾æ•°é‡
            
            # æå–åˆ›å§‹äººè¯„è®ºï¼ˆè¯„è®ºåŒºç¬¬ä¸€æ¡ï¼‰
            founder_comment = self._extract_founder_comment_enhanced(soup)
            if founder_comment:
                details['founder_comment'] = founder_comment
            
        except Exception as e:
            logger.warning(f"ä»é¡µé¢æå–è¯¦æƒ…å¤±è´¥: {e}")
        
        return details
    
    def _extract_founder_comment_enhanced(self, soup: BeautifulSoup) -> Optional[str]:
        """æå–åˆ›å§‹äººè¯„è®ºçš„å¢å¼ºç‰ˆæœ¬"""
        try:
            # æŸ¥æ‰¾è¯„è®ºåŒºçš„å¤šç§å¯èƒ½ä½ç½®
            comment_selectors = [
                '.comment',
                '[data-testid="comment"]',
                '.maker-comment',
                '.founder-comment',
                '.discussion',
                '.reviews'
            ]
            
            for selector in comment_selectors:
                elements = soup.select(selector)
                if elements:
                    # è·å–ç¬¬ä¸€æ¡è¯„è®º
                    first_comment = elements[0].get_text(strip=True)
                    if len(first_comment) > 100:
                        logger.info(f"ä»é¡µé¢è§£æåˆ°åˆ›å§‹äººè¯„è®º: {first_comment[:100]}...")
                        return first_comment
        except:
            pass
        
        return None

    def _get_product_details(self, product_url: str) -> Dict[str, Any]:
        """è·å–äº§å“è¯¦ç»†é¡µé¢ä¿¡æ¯ï¼ŒåŒ…æ‹¬åˆ›å§‹äººè¯„è®º"""
        details = {}
        
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'en-US,en;q=0.5',
                'Accept-Encoding': 'gzip, deflate',
                'Connection': 'keep-alive',
            }
            
            logger.info(f"è·å–äº§å“è¯¦æƒ…: {product_url}")
            response = requests.get(product_url, headers=headers, timeout=15)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # æå–äº§å“åç§°
            title_element = soup.find('h1')
            if title_element:
                details['name'] = title_element.get_text(strip=True)
            else:
                # ä»URLæå–äº§å“åç§°
                product_slug = product_url.split('/products/')[-1]
                details['name'] = product_slug.replace('-', ' ').title()
            
            # æå–äº§å“æ ‡è¯­
            tagline_selectors = [
                'h2', 
                '.tagline',
                '[data-test="tagline"]',
                'div[data-test="post-description"]'
            ]
            
            for selector in tagline_selectors:
                tagline_element = soup.select_one(selector)
                if tagline_element:
                    tagline_text = tagline_element.get_text(strip=True)
                    if len(tagline_text) < 200 and tagline_text != details.get('name', ''):
                        details['tagline'] = tagline_text
                        break
            
            # æå–äº§å“æè¿°
            description_elem = soup.find('div', {'data-test': 'post-description'})
            if description_elem:
                details['description'] = description_elem.get_text().strip()
            
            # æå–äº§å“ç½‘ç«™é“¾æ¥
            website_selectors = [
                'a[href*="http"]:not([href*="producthunt"])',
                'a:contains("Visit")',
                'a[data-test="visit-button"]'
            ]
            
            for selector in website_selectors:
                if ':contains(' in selector:
                    # æŸ¥æ‰¾åŒ…å«"Visit"æ–‡æœ¬çš„é“¾æ¥
                    visit_links = soup.find_all('a', string=lambda text: text and 'visit' in text.lower())
                    for link in visit_links:
                        href = link.get('href')
                        if href and 'http' in href and 'producthunt' not in href:
                            details['website'] = href
                            break
                else:
                    website_elem = soup.select_one(selector)
                    if website_elem:
                        href = website_elem.get('href')
                        if href and 'http' in href and 'producthunt' not in href:
                            details['website'] = href
                            break
            
            # æå–æŠ•ç¥¨æ•°
            vote_elements = soup.find_all(text=lambda text: text and text.strip().isdigit())
            for vote_text in vote_elements:
                try:
                    vote_num = int(vote_text.strip())
                    if 10 <= vote_num <= 9999:  # åˆç†çš„ç¥¨æ•°èŒƒå›´
                        details['votes'] = vote_num
                        break
                except:
                    continue
            
            # æå–æ ‡ç­¾
            tag_elements = soup.find_all('a', href=lambda x: x and '/categories/' in x)
            tags = []
            for tag_elem in tag_elements:
                tag_text = tag_elem.get_text(strip=True)
                if tag_text and tag_text not in tags:
                    tags.append(tag_text)
            details['tags'] = tags
            
            # æå–åˆ›å§‹äººè¯„è®ºï¼ˆåŸæœ‰å­—æ®µï¼‰
            maker_comment_elem = soup.find('div', {'data-test': 'maker-comment'})
            if maker_comment_elem:
                details['maker_comment'] = maker_comment_elem.get_text().strip()
            
            # æ–°å¢ï¼šæå–è¯„è®ºåŒºçš„ç¬¬ä¸€æ¡è¯„è®ºï¼ˆé€šå¸¸æ˜¯åˆ›å§‹äººçš„è¯„è®ºï¼‰
            founder_comment = self._extract_founder_comment(soup)
            if founder_comment:
                details['founder_comment'] = founder_comment
                # å¦‚æœæ²¡æœ‰maker_commentï¼Œä½¿ç”¨founder_commentä½œä¸ºmaker_comment
                if not details.get('maker_comment'):
                    details['maker_comment'] = founder_comment
                logger.info(f"æˆåŠŸæå–åˆ›å§‹äººè¯„è®º: {len(founder_comment)} å­—ç¬¦")
            else:
                logger.warning("æœªæ‰¾åˆ°åˆ›å§‹äººè¯„è®º")
            
            # æå–æˆªå›¾
            screenshot_elem = soup.find('img', {'data-test': 'post-screenshot'})
            if screenshot_elem:
                details['screenshot_url'] = screenshot_elem.get('src')
                
            logger.info(f"æˆåŠŸè·å–äº§å“è¯¦æƒ…: {details.get('name', 'Unknown')}")
                
        except Exception as e:
            logger.warning(f"è·å–äº§å“è¯¦ç»†ä¿¡æ¯å¤±è´¥ {product_url}: {e}")
            
        return details
    
    def _extract_founder_comment(self, soup: BeautifulSoup) -> Optional[str]:
        """ä»é¡µé¢ä¸­æå–åˆ›å§‹äººçš„è¯„è®ºï¼ˆè¯„è®ºåŒºç¬¬ä¸€æ¡ï¼‰"""
        try:
            logger.info("å¼€å§‹æå–åˆ›å§‹äººè¯„è®º...")
            
            # æ–¹æ³•1: æŸ¥æ‰¾å·²çŸ¥çš„åˆ›å§‹äººè¯„è®ºæ¨¡å¼ï¼ˆåŸºäºTwentyçš„å®é™…é¡µé¢ç»“æ„ï¼‰
            # æœç´¢åŒ…å«ç‰¹å®šå¼€åœºç™½çš„æ–‡æœ¬
            opening_phrases = [
                "Hey everyone",
                "Hi everyone", 
                "Hello everyone",
                "Thomas here, co-founder",
                "here, co-founder",
                "co-founder of"
            ]
            
            # éå†æ‰€æœ‰æ–‡æœ¬èŠ‚ç‚¹æŸ¥æ‰¾åˆ›å§‹äººè¯„è®º
            all_text_elements = soup.find_all(text=True)
            
            for text_node in all_text_elements:
                text = text_node.strip()
                # æ£€æŸ¥æ˜¯å¦åŒ…å«å¼€åœºç™½
                if any(phrase in text for phrase in opening_phrases) and len(text) > 200:
                    # è·å–åŒ…å«è¿™ä¸ªæ–‡æœ¬çš„çˆ¶å…ƒç´ 
                    parent = text_node.parent
                    if parent:
                        full_text = parent.get_text(strip=True)
                        if self._looks_like_founder_comment(full_text):
                            logger.info(f"æ–¹æ³•1æ‰¾åˆ°åˆ›å§‹äººè¯„è®º: {len(full_text)} å­—ç¬¦")
                            return full_text
            
            # æ–¹æ³•2: æŸ¥æ‰¾åŒ…å«"Maker"æ ‡è¯†çš„è¯„è®º
            # åœ¨Product Huntä¸Šï¼Œåˆ›å§‹äººé€šå¸¸ä¼šæœ‰"Maker"æ ‡è¯†
            maker_elements = soup.find_all(text=lambda text: text and 'Maker' in text)
            
            for maker_elem in maker_elements:
                # æŸ¥æ‰¾Makerå…ƒç´ é™„è¿‘çš„å¤§æ®µæ–‡æœ¬
                current = maker_elem.parent
                for _ in range(5):  # å‘ä¸ŠæŸ¥æ‰¾5å±‚çˆ¶å…ƒç´ 
                    if current:
                        comment_text = current.get_text(strip=True)
                        if (len(comment_text) > 200 and 
                            self._looks_like_founder_comment(comment_text)):
                            logger.info(f"æ–¹æ³•2æ‰¾åˆ°åˆ›å§‹äººè¯„è®º: {len(comment_text)} å­—ç¬¦")
                            return comment_text
                        current = current.parent
            
            # æ–¹æ³•3: æŸ¥æ‰¾é•¿æ–‡æœ¬å—ä¸­åŒ…å«åˆ›å§‹äººå…³é”®è¯çš„å†…å®¹
            founder_keywords = [
                'co-founder', 'founder', 'CEO', 'created', 'built this',
                'we built', 'I built', 'our team', 'my team', 'startup'
            ]
            
            # æŸ¥æ‰¾æ‰€æœ‰çš„divå…ƒç´ 
            all_divs = soup.find_all(['div', 'p', 'section'])
            
            for element in all_divs:
                text = element.get_text(strip=True)
                if len(text) > 200:  # è¶³å¤Ÿé•¿çš„æ–‡æœ¬
                    text_lower = text.lower()
                    # æ£€æŸ¥æ˜¯å¦åŒ…å«å¤šä¸ªåˆ›å§‹äººå…³é”®è¯
                    keyword_count = sum(1 for keyword in founder_keywords if keyword in text_lower)
                    
                    if keyword_count >= 2 and self._looks_like_founder_comment(text):
                        logger.info(f"æ–¹æ³•3æ‰¾åˆ°åˆ›å§‹äººè¯„è®º: {len(text)} å­—ç¬¦")
                        return text
            
            # æ–¹æ³•4: ç‰¹æ®Šå¤„ç† - æŸ¥æ‰¾é¡µé¢ä¸­æœ€é•¿çš„åŒ…å«åˆ›å§‹äººä¿¡æ¯çš„æ–‡æœ¬
            all_elements = soup.find_all(['div', 'p', 'span'])
            best_candidate = None
            best_score = 0
            
            for element in all_elements:
                text = element.get_text(strip=True)
                if len(text) > 100:
                    score = 0
                    text_lower = text.lower()
                    
                    # æ‰“åˆ†ç³»ç»Ÿ
                    if 'co-founder' in text_lower or 'founder' in text_lower:
                        score += 3
                    if 'hey everyone' in text_lower or 'hi everyone' in text_lower:
                        score += 3
                    if 'we built' in text_lower or 'I built' in text_lower:
                        score += 2
                    if 'startup' in text_lower:
                        score += 1
                    if len(text) > 500:
                        score += 2
                    
                    if score > best_score and self._looks_like_founder_comment(text):
                        best_score = score
                        best_candidate = text
            
            if best_candidate:
                logger.info(f"æ–¹æ³•4æ‰¾åˆ°åˆ›å§‹äººè¯„è®º: {len(best_candidate)} å­—ç¬¦")
                return best_candidate
            
            logger.warning("æ‰€æœ‰æ–¹æ³•éƒ½æœªæ‰¾åˆ°åˆ›å§‹äººè¯„è®º")
            return None
            
        except Exception as e:
            logger.error(f"æå–åˆ›å§‹äººè¯„è®ºå¤±è´¥: {e}")
            return None
    
    def _looks_like_founder_comment(self, text: str) -> bool:
        """åˆ¤æ–­æ–‡æœ¬æ˜¯å¦çœ‹èµ·æ¥åƒåˆ›å§‹äººçš„è¯„è®º"""
        try:
            text_lower = text.lower()
            
            # åˆ›å§‹äººè¯„è®ºçš„ç‰¹å¾
            founder_indicators = [
                'co-founder', 'founder', 'ceo', 'created', 'built this', 'we built',
                'i built', 'our team', 'my team', 'started', 'launched',
                'hey everyone', 'hi everyone', 'hello everyone'
            ]
            
            # æ’é™¤çš„ç‰¹å¾ï¼ˆå¯èƒ½æ˜¯å…¶ä»–ç±»å‹çš„å†…å®¹ï¼‰
            exclusion_indicators = [
                'advertisement', 'sponsored', 'terms of service', 'privacy policy',
                'copyright', 'Â©', 'all rights reserved', 'loading', 'error'
            ]
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«åˆ›å§‹äººæŒ‡ç¤ºè¯
            has_founder_indicator = any(indicator in text_lower for indicator in founder_indicators)
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«æ’é™¤æŒ‡ç¤ºè¯
            has_exclusion = any(indicator in text_lower for indicator in exclusion_indicators)
            
            # é•¿åº¦æ£€æŸ¥ï¼šå¤ªçŸ­çš„ä¸å¤ªå¯èƒ½æ˜¯è¯¦ç»†çš„åˆ›å§‹äººä»‹ç»
            is_reasonable_length = 50 <= len(text) <= 5000
            
            return has_founder_indicator and not has_exclusion and is_reasonable_length
            
        except Exception:
            return False
    
    def is_ai_related(self, product: ProductInfo) -> bool:
        """
        ç®€å•åˆ¤æ–­äº§å“æ˜¯å¦ä¸AIç›¸å…³
        æ›´è¯¦ç»†çš„AIåˆ†æåœ¨ai_analyzeræ¨¡å—ä¸­è¿›è¡Œ
        """
        ai_keywords = config.filtering.ai_keywords
        text_to_check = f"{product.name} {product.tagline} {product.description}".lower()
        
        return any(keyword.lower() in text_to_check for keyword in ai_keywords) 